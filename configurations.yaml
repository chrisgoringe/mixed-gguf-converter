'metadata':
  'native': Q4_1,Q5_1,Q8_0
'configurations':
  '5_1':
    casts:
    - castto: Q4_1
      layers: 0-56
    notes: full Q4_1 quantization
  '5_3':
    casts:
    - castto: Q5_1
      layers: 0-2, 4, 6, 8-9, 11-13, 49-54
    - castto: Q4_1
      layers: 3, 5, 7, 10, 14, 16-29, 32-33, 42, 44-48, 55-56
    - castto: Q3_K_S
      layers: 15, 30-31, 34-41, 43
    notes: pretty small
  '5_9':
    casts:
    - castto: Q5_1
      layers: 0-25, 27-28, 44-54
    - castto: Q4_1
      layers: 26, 29-43, 55-56
    notes: should work on 12GB card
  '7_4':
    casts:
    - castto: BF16
      layers: 0-2
    - castto: Q8_0
      layers: 5, 7-12
    - castto: Q5_1
      layers: 3-4, 6, 13-33, 42-55
    - castto: Q4_1
      layers: 34-41, 56
    notes: roughly same size as 8bit model
  '8_4':
    casts:
    - castto: BF16
      layers: 0-4, 10
    - castto: Q8_0
      layers: 5-9, 11-14
    - castto: Q5_1
      layers: 15-35, 41-55
    - castto: Q4_1
      layers: 36-40, 56
    notes: good balance for 16GB card
  '9_2':
    casts:
    - castto: BF16
      layers: 0-8, 10, 12
    - castto: Q6_K
      layers: 9, 11, 13-21, 49-54
    - castto: Q5_K_S
      layers: 22-34, 41-48, 55
    - castto: Q4_K_S
      layers: 35-40
    - castto: Q4_1
      layers: '56'
    notes: Perfect for 16GB cards
  '9_6':
    casts:
    - castto: BF16
      layers: 0-10
    - castto: Q8_0
      layers: 11-14, 54
    - castto: Q5_1
      layers: 15-36, 39-53, 55
    - castto: Q4_1
      layers: 37-38, 56
    notes: might just fit on a 16GB card
